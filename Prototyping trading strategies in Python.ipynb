{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping trading strategies in Python\n",
    "\n",
    "This workshop will introduce a simple idea for finding inefficient closing auctions. The idea is based on the assumption that some market participants enter their positions in the morning and exit them in the closing auction. If more participants enter the same positions (long or short) it might make the closing price inefficient. We could exploit that by entering a position in the closing auction and exiting the next day in the opening auction.\n",
    "\n",
    "We will learn how to:\n",
    "\n",
    "- download data from [finance.google.com](https://finance.google.com);\n",
    "- transform data using the [Pandas](https://pandas.pydata.org) library;\n",
    "- construct a simple strategy;\n",
    "- test and optimise it on historical data;\n",
    "- plot and interpret the results.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer #1 (from our legal team with â¤ï¸)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEGAL NOTICE / DISCLAIMER\n",
    "\n",
    "This presentation (the â€œPresentationâ€) has been prepared by WOOD & Company Financial Services, a.s., (â€œWOODâ€) and Quantlane s.r.o. (â€œQuantlaneâ€) solely for educational purposes. The Presentation may not be distributed, reproduced, or used for any other purposes. The Presentation is of purely educational nature and has no legal value.\n",
    "\n",
    "No representation or warranty, express or implied, is made and no responsibility is or will be accepted by WOOD or Quantlane for the accuracy, reliability or completeness of any data used or presented in this Presentation. The Presentation does not purport to contain all information which may be material for the subject herein.\n",
    "\n",
    "Neither the receipt of the Presentation, nor any information subsequently provided in connection with it shall be relied upon as constituting advice, especially investment advice, to the recipients. Neither WOOD and/or Quantlane, nor its respective directors, agents or employees, accept any liability to any person in relation to the distribution or possession of the Presentation.\n",
    "\n",
    "The Presentation is neither an offer nor the solicitation of an offer to sell or purchase any investment. All estimates, opinions and other information contained herein are subject to change without notice and are provided in good faith but without legal responsibility or liability. Opinion may be personal to the author and may not reflect the opinions of WOOD or Quantlane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't trade this at home.** We are making a lot of simplifying assumptions here. The purpose of this workshop is to give you a taste of what it's like to verify trading ideas in Python. The purpose is _not_ to give you a working strategy, or even a working idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's come up with a trading strategy ðŸ’¡âœ¨\n",
    "\n",
    "## First, some background on stock market auctions\n",
    "Every trading day stock markets enter regimes called _auctions_. There is usually one opening and one closing auction per stock per day. Sometimes there are scheduled intraday auctions, and sometimes unscheduled auctions are triggered by unusual market circumstances. \n",
    "\n",
    "### Opening auction\n",
    "The opening auction provides market participants with means to place buy and sell orders knowing that they will be (potentially) matched at some predefined time, e.g. 9.00am. There is no trading possible before the auction. Standard continuous trading usually starts immediatelly after the opening auction.\n",
    "\n",
    "### Closing auction\n",
    "There is a closing auction at the end of every trading day in each stock and exchange. This means that every market participant can enter buy and sell orders such that all the orders are matched at a certain, possibly randomised time.\n",
    "\n",
    "### Matching orders\n",
    "Orders are matched at a price that will maximise the total quantity traded in the auction. Orders are filled if they have a counterparty:\n",
    "\n",
    "- If you want to buy for a price that is below the auction price you are not filled.\n",
    "- If you want to buy for a price that is above the auction price you are filled.\n",
    "- If you want to buy for a price that is exactly the auction price you might be filled or not, depending on your queue position in the market.\n",
    "\n",
    "### Recommended reading\n",
    "- https://www.investopedia.com/articles/investing/091113/auction-method-how-nyse-stock-prices-are-set.asp\n",
    "- https://www.nyse.com/publicdocs/nyse/markets/nyse/NYSE_Opening_and_Closing_Auctions_Fact_Sheet.pdf\n",
    "\n",
    "\n",
    "## Our simple strategy\n",
    "\n",
    "**Remember that there should be a fundamental, robust idea behind every strategy.** Blind data mining yields only shortlived results.\n",
    "\n",
    "The fundamental idea is based on an assumption that some market participants enter their positions in the morning and exit them in the closing auction. (Momentum traders that do not want to hold overnight positions.) If more participants enter the same positions (long or short) it might make the closing price inefficient. We could exploit that by entering a position in the closing auction and exiting the next day in the opening auction.\n",
    "\n",
    "What we need for this for this strategy is to figure out some efficient price. We decided to use Volume Weighted Average Price (VWAP) in a time interval before the closing time. Let $T$ be the closing time. Our VWAP shall span from $T - \\delta_1$ to $T - \\delta_2$ where $\\delta_1 > \\delta_2$.\n",
    "\n",
    "$$\n",
    "VWAP = \\frac{\\sum_{\\delta \\in (\\delta_1, \\delta_2)} p_{T-\\delta} \\cdot v_{T-\\delta}}{\\sum_{\\delta \\in (\\delta_1, \\delta_2)} v_{T-\\delta}}\n",
    "$$\n",
    "\n",
    "where $p_t$ / $v_t$ is price / volume traded at time $t$.\n",
    "\n",
    "We do not use data in $(T-\\delta_2, T)$ because the same those market participants that might move the price in the closing auction might also move it in last few moments before the auction.\n",
    "\n",
    "To keep this tutorial simple, we pick a fixed $\\delta_1$ and $\\delta_2$.\n",
    "\n",
    "The strategy enters a long trade when the price in the closing auction is below the VWAP by some margin, but not too much. Similarly it enters short trades. It exits its positions in next day opening auctions. These margins (minimal and maximal price inefficiency) are our parameters of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inefficiency_example\n",
    "inefficiency_example.generate_and_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { min-width: 90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterator, List, Set\n",
    "\n",
    "import concurrent.futures\n",
    "import csv\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import requests\n",
    "import scipy\n",
    "import statsmodels.stats.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data\n",
    "\n",
    "There are several places to get free data. You can use finance.yahoo.com, finance.google.com and many more to get _daily_ (end-of-day) data. \n",
    "\n",
    "Getting free _minute_ data seems to be harder. We found free data for you only at finance.google.com, though the (undocumented) API only returns the last 15 business days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_google_finance_data_for_instrument(instrument: str, lookback_days: int) -> List[str]:\n",
    "    # Be aware that changing TIME_INTERVAL will most likely result in an unpredictable result (weirdly looking data).\n",
    "    TIME_INTERVAL = 60\n",
    "    url = 'http://finance.google.com/finance/getprices'\n",
    "    params = {\n",
    "        'i': TIME_INTERVAL,\n",
    "        'p': f'{lookback_days}d',\n",
    "        'f': 'd,o,h,l,c,v', # datetime OR timedelta, open, high, low, close, volume\n",
    "        'q': instrument,\n",
    "    }\n",
    "    response = requests.get(url, params = params)\n",
    "    response.raise_for_status()\n",
    "    return response.text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_test = download_google_finance_data_for_instrument('AAPL', 2)\n",
    "aapl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_google_finance_data_for_instrument(instrument: str, lines: List[str]) -> Iterator[List[Any]]:\n",
    "    '''\n",
    "    We expect each CSV row to be in the form [timestamp or timedelta, 'open', 'high', 'low', 'close', 'volume'].\n",
    "    We skip all rows until we find the starting timestamp in the form 'a1512570600'.\n",
    "    '''\n",
    "    starting_datetime: datetime.datetime = None\n",
    "    for row in csv.reader(lines):\n",
    "        if row[0][0] == 'a':\n",
    "            starting_datetime = datetime.datetime.fromtimestamp(float(row[0][1:]))\n",
    "            yield [starting_datetime, instrument] + row[1:]\n",
    "        elif starting_datetime is not None and not row[0].startswith('TIMEZONE_OFFSET='):\n",
    "            timedelta = datetime.timedelta(minutes = float(row[0]))\n",
    "            yield [starting_datetime + timedelta, instrument] + row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_test_parsed = list(parse_google_finance_data_for_instrument('AAPL', aapl_test))\n",
    "aapl_test_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsed_google_finance_data_to_dataframe(rows: Iterator[str]) -> pd.DataFrame:\n",
    "    dataframe = pd.DataFrame(data = list(rows), columns = ('time', 'instrument', 'open', 'high', 'low', 'close', 'volume'))\n",
    "    dataframe = dataframe.sort_values(['instrument', 'time'])\n",
    "    dataframe[['open', 'high', 'low', 'close']] = dataframe[['open', 'high', 'low', 'close']].astype(float)\n",
    "    dataframe['volume'] = dataframe['volume'].astype(int)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_google_finance_data_to_dataframe(aapl_test_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_finance_data_in_parallel(instruments: Set[str], lookback_days: int) -> pd.DataFrame:\n",
    "    pipeline = lambda instrument: parse_google_finance_data_for_instrument(\n",
    "        instrument,\n",
    "        download_google_finance_data_for_instrument(instrument, lookback_days)\n",
    "    )\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        rows = itertools.chain.from_iterable(executor.map(pipeline, instruments))\n",
    "        # Key performance trick: we only convert to DataFrame once we have _all_ rows downloaded and parsed.\n",
    "        return parsed_google_finance_data_to_dataframe(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aapl_and_msft_test = get_google_finance_data_in_parallel({'AAPL', 'MSFT'}, 2)\n",
    "aapl_and_msft_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_and_msft_test.to_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head 'data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_offline_data(instruments: Set[str], lookback_days: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    Loads data from any CSVs stored in the 'data' folder and transforms them \n",
    "    to the same type we get from :func:`parsed_google_finance_data_to_dataframe`.\n",
    "    '''\n",
    "    all_dataframes = [\n",
    "        pd.read_csv(os.path.join('data', filename), index_col = 0)\n",
    "        for filename in os.listdir('data')\n",
    "        if filename.endswith('.csv')\n",
    "    ]\n",
    "    combined = pd.concat(all_dataframes)\n",
    "    # Reorder columns if needed:\n",
    "    combined = combined[['time', 'instrument', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    combined['time'] = combined['time'].map(pd.Timestamp)\n",
    "    combined = combined.sort_values(['instrument', 'time']).drop_duplicates().reset_index()\n",
    "    \n",
    "    cutoff_date = pd.Timestamp(datetime.date.today() - datetime.timedelta(days = lookback_days))\n",
    "    filtered = combined[\n",
    "        (combined['instrument'].isin(instruments))\n",
    "        & (combined['time'] >= cutoff_date)\n",
    "    ]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_offline_data({'AAPL'}, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's get the entire S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_instruments = {\n",
    "    'A', 'AAL', 'AAP', 'AAPL', 'ABBV', 'ABC', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADS', 'ADSK', 'AEE', 'AEP', \n",
    "    'AES', 'AET', 'AFL', 'AGN', 'AIG', 'AIV', 'AIZ', 'AJG', 'AKAM', 'ALB', 'ALGN', 'ALK', 'ALL', 'ALLE', 'ALXN',\n",
    "    'AMAT', 'AMD', 'AME', 'AMG', 'AMGN', 'AMP', 'AMT', 'AMZN', 'ANDV', 'ANSS', 'ANTM', 'AON', 'AOS', 'APA', 'APC', \n",
    "    'APD', 'APH', 'ARE', 'ARNC', 'ATVI', 'AVB', 'AVGO', 'AVY', 'AWK', 'AXP', 'AYI', 'AZO', 'B', 'BA', 'BAC', 'BAX',\n",
    "    'BBT', 'BBY', 'BCR', 'BDX', 'BEN', 'BHF', 'BHGE', 'BIIB', 'BK', 'BLK', 'BLL', 'BMY', 'BR', 'BSX', 'BWA', 'BXP',\n",
    "    'C', 'CA', 'CAG', 'CAH', 'CAT', 'CB', 'CBG', 'CBOE', 'CBS', 'CCI', 'CCL', 'CDNS', 'CELG', 'CERN', 'CF', 'CFG',\n",
    "    'CHD', 'CHKCVX', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX', 'CMA', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', \n",
    "    'CNP', 'COF', 'COG', 'COL', 'COO', 'COP', 'COST', 'COTY', 'CPB', 'CRM', 'CSCO', 'CSRA', 'CSX', 'CTAS', 'CTL', \n",
    "    'CTSH', 'CTXS', 'CVS', 'CXO', 'D', 'DAL', 'DE', 'DFS', 'DG', 'DGX', 'DHI', 'DHR', 'DIS', 'DISCA', 'DISCK', \n",
    "    'DISH', 'DLPH', 'DLR', 'DLTR', 'DOV', 'DPS', 'DRE', 'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DWDP', 'DXC', 'EA', \n",
    "    'EBAY', 'ECL', 'ED', 'EFX', 'EIX', 'EL', 'EMN', 'EMR', 'EOG', 'EQIX', 'EQR', 'EQT', 'ES', 'ESRX', 'ESS', 'ETFC', \n",
    "    'ETN', 'ETR', 'EVHC', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FAST', 'FB', 'FBHS', 'FCX', 'FDX', 'FE', 'FFIV',\n",
    "    'FIS', 'FISV', 'FITB', 'FL', 'FLIR', 'FLR', 'FLS', 'FMC', 'FOX', 'FOXA', 'FRT', 'FTI', 'FTV', 'GD', 'GE', 'GGP',\n",
    "    'GILD', 'GIS', 'GLW', 'GM', 'GOOG', 'GOOGL', 'GPC', 'GPN', 'GPS', 'GRMN', 'GS', 'GT', 'GWW', 'HAL', 'HAS', 'HBAN',\n",
    "    'HBI', 'HCA', 'HCN', 'HCP', 'HD', 'HES', 'HIG', 'HLT', 'HOG', 'HOLX', 'HON', 'HP', 'HPE', 'HPQ', 'HRB', 'HRL', \n",
    "    'HRS', 'HSIC', 'HST', 'HSY', 'HUM', 'IBM', 'ICE', 'IDXX', 'IFF', 'ILMN', 'INCY', 'INFO', 'INTC', 'INTU', 'IP', \n",
    "    'IPG', 'IQV', 'IR', 'IRM', 'ISRG', 'IT', 'ITW', 'IVZ', 'JBHT', 'JCI', 'JEC', 'JNJ', 'JNPR', 'JPM', 'JWN', 'K',\n",
    "    'KEY', 'KHC', 'KIM', 'KLAC', 'KMB', 'KMI', 'KMX', 'KO', 'KORS', 'KR', 'KSS', 'KSU', 'L', 'LB', 'LEG', 'LEN', 'LH',\n",
    "    'LKQ', 'LLL', 'LLY', 'LMT', 'LNC', 'LNT', 'LOW', 'LRCX', 'LUK', 'LUV', 'LYB', 'M', 'MA', 'MAA', 'MAC', 'MAR', \n",
    "    'MAS', 'MAT', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', 'MHK', 'MKC', 'MLM', 'MMC', 'MMM', 'MNST',\n",
    "    'MO', 'MON', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', 'MSI', 'MTB', 'MTD', 'MU', 'MYL', 'NAVI', 'NBL', 'NCLH',\n",
    "    'NDAQ', 'NEE', 'NEM', 'NFLX', 'NFX', 'NI', 'NKE', 'NLSN', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE',\n",
    "    'NVDA', 'NWL', 'NWS', 'NWSA', 'O', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PCAR', 'PCG', 'PCLN',\n",
    "    'PDCO', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', 'PNR', 'PNW',\n",
    "    'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'PX', 'PXD', 'PYPL', 'QCOM', 'QRVO', 'RCL', 'RE', 'REG',\n",
    "    'REGN', 'RF', 'RHI', 'RHT', 'RJF', 'RL', 'RMD', 'ROK', 'ROP', 'ROST', 'RRC', 'RSG', 'RTN', 'SBAC', 'SBUX', 'SCG',\n",
    "    'SCHW', 'SEE', 'SHW', 'SIG', 'SJM', 'SLB', 'SLG', 'SNA', 'SNI', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRCL', 'SRE', 'STI',\n",
    "    'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYMC', 'SYY', 'T', 'TAP', 'TDG', 'TEL', 'TGT', 'TIF', 'TJX',\n",
    "    'TMK', 'TMO', 'TPR', 'TRIP', 'TROW', 'TRV', 'TSCO', 'TSN', 'TSS', 'TWX', 'TXN', 'TXT', 'UA', 'UAA', 'UAL', 'UDR',\n",
    "    'UHS', 'ULTA', 'UNH', 'UNM', 'UNP', 'UPS', 'URI', 'USB', 'UTX', 'V', 'VAR', 'VFC', 'VIAB', 'VLO', 'VMC', 'VNO', \n",
    "    'VRSK', 'VRSN', 'VRTX', 'VTR', 'VZ', 'WAT', 'WBA', 'WDC', 'WEC', 'WFC', 'WHR', 'WLTW', 'WM', 'WMB', 'WMT', 'WRK', \n",
    "    'WU', 'WY', 'WYN', 'WYNN', 'XEC', 'XEL', 'XL', 'XLNX', 'XOM', 'XRAY', 'XRX', 'XYL', 'YUM', 'ZBH', 'ZION', 'ZTS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_sp500_data_online = get_google_finance_data_in_parallel(sp500_instruments, 15)\n",
    "# len(raw_sp500_data_online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sp500_data_offline = load_offline_data(sp500_instruments, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# raw_sp500_data = pd.concat([raw_sp500_data_online, raw_sp500_data_offline]).drop_duplicates().sort_values(['time', 'instrument']).reset_index(drop = True)\n",
    "raw_sp500_data = raw_sp500_data_offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sp500_data['date'] = raw_sp500_data['time'].map(lambda timestamp: timestamp.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sp500_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(raw_minute_data: pd.DataFrame, delta_1: int, delta_2: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gets vwap from \"delta_1\" seconds before close up to \"delta_2\" seconds before yesterday close\n",
    "    and joins it with the yesterday closing price and opening price today open price.\n",
    "    '''\n",
    "    aggregated = raw_minute_data.groupby(['date', 'instrument']).apply(\n",
    "        lambda df_per_day_and_instrument: \n",
    "            summarise_data_per_day_and_instrument(\n",
    "                minute_data = df_per_day_and_instrument, \n",
    "                delta_1 = delta_1, \n",
    "                delta_2 = delta_2\n",
    "            )\n",
    "    )\n",
    "    return aggregated.reset_index().groupby('instrument').apply(prev_day_info).dropna()\n",
    "\n",
    "\n",
    "def summarise_data_per_day_and_instrument(minute_data: pd.DataFrame, delta_1: int, delta_2: int) -> pd.Series:\n",
    "    '''\n",
    "    Extract the first (opening) price, last (closing) price, and VWAP from the interval\n",
    "    (closing_time - delta_1 seconds, closing_time - delta_2) from ``minute_data``.\n",
    "    \n",
    "    We know that the ``minute_data`` dataframe only contains minute data for one instrument \n",
    "    and one day.\n",
    "    \n",
    "    :param minute_data: dataframe with columns 'open', 'close' and 'time' at least. \n",
    "    :param delta_1: number of seconds before closing price, has to be higher than delta_2\n",
    "    :param delta_2: number of seconds before closing price, has to be smaller than delta_1\n",
    "    '''\n",
    "    if delta_1 <= delta_2:\n",
    "        raise ValueError('delta_1 cannot be smaller or equal than delta_2')\n",
    "\n",
    "    delta_1 = pd.Timedelta(delta_1, unit = 's')\n",
    "    delta_2 = pd.Timedelta(delta_2, unit = 's')    \n",
    "    closing_time = minute_data.iloc[-1].time\n",
    "    vwap_interval = minute_data.loc[\n",
    "        (minute_data.time > closing_time - delta_1) \n",
    "        & (minute_data.time < closing_time - delta_2)\n",
    "    ]\n",
    "    vwap = (vwap_interval.close * vwap_interval.volume).sum() / vwap_interval.volume.sum()\n",
    "    transformation = {\n",
    "        'opening_price': minute_data.open.iloc[0],\n",
    "        'vwap': vwap,\n",
    "        'closing_price': minute_data.close.iloc[-1],\n",
    "    }\n",
    "    return pd.Series(transformation)\n",
    "\n",
    "\n",
    "def prev_day_info(summarised_daily_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Returns dataframe where for given date it gets the VWAP and closing price \n",
    "    from the previous day.\n",
    "    \n",
    "    :param summarised_daily_data: dataframe over only one instrument\n",
    "    '''\n",
    "    transformation = {\n",
    "        'date': summarised_daily_data.date,\n",
    "        'instrument': summarised_daily_data.instrument,\n",
    "        'prev_day_closing_price': summarised_daily_data.closing_price.shift(),\n",
    "        'prev_day_vwap': summarised_daily_data.vwap.shift(),\n",
    "        'opening_price': summarised_daily_data.opening_price\n",
    "        \n",
    "    }\n",
    "    return pd.DataFrame(transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_sp500_data = transform_data(raw_sp500_data, delta_1 = 1800, delta_2 = 300)\n",
    "transformed_sp500_data['relative_diff'] = transformed_sp500_data['prev_day_closing_price'] / transformed_sp500_data['prev_day_vwap'] - 1\n",
    "transformed_sp500_data['over_night_change'] = transformed_sp500_data['opening_price'] / transformed_sp500_data['prev_day_closing_price'] - 1\n",
    "transformed_sp500_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_sp500_data.sort_values(['instrument', 'date']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing potential profits\n",
    "\n",
    "We look at profits in relative terms (returns).\n",
    "\n",
    "Be aware that we do not account for dividends in the profit analysis below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what the usual overnight returns are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import returns_plot\n",
    "returns_plot.plot_afternoon_returns_on_morning_returns_error_bar(\n",
    "    results = transformed_sp500_data,\n",
    "    epsilon = 0.001,\n",
    "    width = 0.02,\n",
    "    empirical_distribution_returns = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see what our expected profit would be if we only traded some of those situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_signals_and_calculate_profit(data: pd.DataFrame, min_relative_inefficiency: float, max_relative_inefficiency: float) -> None:\n",
    "    data['signal'] = 'None'\n",
    "    data.loc[\n",
    "        (data.relative_diff > -max_relative_inefficiency) & (data.relative_diff < -min_relative_inefficiency), \n",
    "        'signal'\n",
    "    ] = 'Buy'\n",
    "    data.loc[\n",
    "        (data.relative_diff > min_relative_inefficiency) & (data.relative_diff < max_relative_inefficiency), \n",
    "        'signal'\n",
    "    ] = 'Sell'\n",
    "    data['profit'] = np.nan\n",
    "    data.loc[data.signal == 'Buy', 'profit'] = data[data.signal == 'Buy'].over_night_change\n",
    "    data.loc[data.signal == 'Sell', 'profit'] = -data[data.signal == 'Sell'].over_night_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we pick the parameters by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_relative_inefficiency = 0.0025\n",
    "max_relative_inefficiency = 0.0250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = transformed_sp500_data.copy()\n",
    "generate_signals_and_calculate_profit(test_data, min_relative_inefficiency, max_relative_inefficiency)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profit_per_day(data: pd.DataFrame) -> None:\n",
    "    profit_per_day = data[~pd.isnull(data['profit'])].sort_values('date').groupby('date')['profit'].mean()\n",
    "    trace = go.Bar(\n",
    "        x = profit_per_day.index,\n",
    "        y = profit_per_day.values\n",
    "    )\n",
    "    iplot({'data': [trace], 'layout': {'title': 'Profit'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_profit_per_day(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, we try to optimise the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Sample / Out Of Sample split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the first two thirds of data In Sample and the remaining one third Out Of Sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_sorted_dates = list(transformed_sp500_data['date'].sort_values().drop_duplicates())\n",
    "last_in_sample_date = unique_sorted_dates[round(len(unique_sorted_dates) * 2 / 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_sample_transformed_sp500_data = transformed_sp500_data[transformed_sp500_data['date'] <= last_in_sample_date]\n",
    "out_of_sample_transformed_sp500_data = transformed_sp500_data[transformed_sp500_data['date'] > last_in_sample_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In Sample starting date: {} and ending date: {}'.format(\n",
    "    in_sample_transformed_sp500_data['date'].min().date(), in_sample_transformed_sp500_data['date'].max().date()\n",
    "))\n",
    "print('Out Of Sample starting date: {} and ending date: {}'.format(\n",
    "    out_of_sample_transformed_sp500_data['date'].min().date(), out_of_sample_transformed_sp500_data['date'].max().date()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_profit(min_relative_inefficiency: float, max_relative_inefficiency: float, data: pd.DataFrame, fee: float) -> float:\n",
    "    '''\n",
    "    Calculate total profit for the given input, adjusted for fees. Liquidity, short fees, risk etc. are blithely ignored.\n",
    "    '''\n",
    "    buy_profits = data[\n",
    "        (data.relative_diff > -max_relative_inefficiency) & (data.relative_diff < -min_relative_inefficiency)\n",
    "    ].over_night_change - fee\n",
    "    sell_profits = -data[\n",
    "        (data.relative_diff > min_relative_inefficiency) & (data.relative_diff < max_relative_inefficiency)\n",
    "    ].over_night_change - fee\n",
    "    \n",
    "    if (buy_profits.count() + sell_profits.count()) > 25:\n",
    "        return buy_profits.sum() + sell_profits.sum()\n",
    "    else:\n",
    "        # Ignore parameters for which we get too few observations.\n",
    "        return float('NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the optimisation space looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_relative_inefficiencies: List[float] = []\n",
    "max_relative_inefficiencies: List[float] = []\n",
    "profits: List[float] = []\n",
    "\n",
    "for min_relative_inefficiency in np.linspace(0, 0.02, num = 100):\n",
    "    for max_relative_inefficiency in np.linspace(min_relative_inefficiency, 0.02, num = 100):\n",
    "        min_relative_inefficiencies.append(min_relative_inefficiency)\n",
    "        max_relative_inefficiencies.append(max_relative_inefficiency)\n",
    "        profit = calc_profit(\n",
    "            min_relative_inefficiency = min_relative_inefficiency, \n",
    "            max_relative_inefficiency = max_relative_inefficiency, \n",
    "            data = in_sample_transformed_sp500_data, \n",
    "            fee = 0.0025\n",
    "        )\n",
    "        profits.append(profit)\n",
    "\n",
    "optimization_space = go.Scatter3d(\n",
    "    x = min_relative_inefficiencies,\n",
    "    y = max_relative_inefficiencies,\n",
    "    z = profits,\n",
    "    mode = 'markers',\n",
    "    marker = {\n",
    "        'size': 3,\n",
    "        'line': {\n",
    "            'color': 'rgba(217, 217, 217, 0.4)',\n",
    "            'width': 0.1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "layout = go.Layout(margin = {'l': 0, 'r': 0, 'b': 0, 't': 0})\n",
    "iplot(go.Figure(data = [optimization_space], layout = layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brute force optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimisation_result = scipy.optimize.brute(\n",
    "    func = lambda x: -calc_profit(\n",
    "        min_relative_inefficiency = x[0], \n",
    "        max_relative_inefficiency = x[1], \n",
    "        data = in_sample_transformed_sp500_data,\n",
    "        fee = 0.0025\n",
    "    ),\n",
    "    ranges = (slice(0, 0.03, 0.001), slice(0, 0.03, 0.001)), \n",
    "    full_output = True,\n",
    "    finish = scipy.optimize.fmin\n",
    ")\n",
    "optimisation_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Least Squares Programming optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "constraints = (\n",
    "    {'type': 'ineq', 'fun': lambda x:  x[0] - x[1]},\n",
    ")\n",
    "\n",
    "optimisation_result = scipy.optimize.minimize(\n",
    "    fun = lambda param: -calc_profit(\n",
    "        min_relative_inefficiency = param[0], \n",
    "        max_relative_inefficiency = param[1], \n",
    "        data = in_sample_transformed_sp500_data,\n",
    "        fee = 0.0025\n",
    "    ),\n",
    "    x0 = (0.0050, 0.0200),\n",
    "    method = 'SLSQP', \n",
    "    bounds = ((0, 0.03), (0, 0.03)),\n",
    "    constraints = constraints,\n",
    ")\n",
    "optimisation_result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the optimised parameters on the Out Of Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_relative_inefficiency, max_relative_inefficiency = optimisation_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out_of_sample_transformed_sp500_data = out_of_sample_transformed_sp500_data.copy()\n",
    "generate_signals_and_calculate_profit(out_of_sample_transformed_sp500_data, min_relative_inefficiency, max_relative_inefficiency)\n",
    "out_of_sample_transformed_sp500_data[out_of_sample_transformed_sp500_data['signal'] != 'None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_profit_per_day(out_of_sample_transformed_sp500_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_profits = out_of_sample_transformed_sp500_data[~pd.isnull(out_of_sample_transformed_sp500_data.profit)].profit\n",
    "oos_profits.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate confidence intervals too, but be aware that we have few observations to draw from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('95% confidence interval from empirical distribution. In percentages. OOS.')\n",
    "print((oos_profits * 100).quantile([0.025, 0.975]).values)\n",
    "print('\\n95% confidence interval from normal distribution. In percentages. OOS.')\n",
    "print(statsmodels.stats.api.DescrStatsW(oos_profits * 100).zconfint_mean(alpha = 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
